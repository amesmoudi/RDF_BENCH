#0 building with "default" instance using docker driver

#1 [base internal] load build definition from Dockerfile
#1 transferring dockerfile: 4.06kB 0.0s done
#1 DONE 0.0s

#2 [base internal] load metadata for docker.io/library/ubuntu:trusty
#2 DONE 0.8s

#3 [base internal] load .dockerignore
#3 transferring context: 2B done
#3 DONE 0.0s

#4 [base  1/23] FROM docker.io/library/ubuntu:trusty@sha256:64483f3496c1373bfd55348e88694d1c4d0c9b660dee6bfef5e12f43b9933b30
#4 DONE 0.0s

#5 [base internal] load build context
#5 transferring context: 25.20kB 0.1s done
#5 DONE 0.1s

#6 [base 19/23] RUN wget --no-check-certificate https://archive.apache.org/dist/spark/spark-1.5.1/spark-1.5.1-bin-hadoop2.6.tgz
#6 CACHED

#7 [base 21/23] RUN wget https://github.com/sbt/sbt/releases/download/v0.13.18/sbt-0.13.18.zip &&     unzip sbt-0.13.18.zip -d /usr/local &&     rm sbt-0.13.18.zip
#7 CACHED

#8 [base  8/23] RUN chmod 600 /root/.ssh/id_rsa
#8 CACHED

#9 [base 22/23] RUN echo "export JAVA_HOME=/usr/lib/jvm/java-7-openjdk-amd64" >> /root/.bashrc     && echo "export HADOOP_HOME=/opt/hadoop-2.6.4" >> /root/.bashrc     && echo "export YARN_CONF_DIR=/opt/hadoop-2.6.4/etc/hadoop/" >> /root/.bashrc     && echo "export HADOOP_CONF_DIR=/opt/hadoop-2.6.4/etc/hadoop/" >> /root/.bashrc     && echo "export SPARK_HOME=/opt/spark-1.5.1-bin-hadoop2.6" >> /root/.bashrc     && echo "export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$SPARK_HOME/bin:$SPARK_HOME/sbin" >> /root/.bashrc
#9 CACHED

#10 [base  2/23] RUN apt-get update && apt-get install -y software-properties-common
#10 CACHED

#11 [base  6/23] RUN mkdir -p /root/.ssh/
#11 CACHED

#12 [base  7/23] ADD ssh/* /root/.ssh/
#12 CACHED

#13 [base  3/23] RUN add-apt-repository ppa:openjdk-r/ppa
#13 CACHED

#14 [base  4/23] RUN apt-get update     && apt install -y arp-scan python3 gnupg2 curl unzip  nano dnsutils iputils-ping net-tools openjdk-8-jre openjdk-8-jdk  ssh git openssh-server      && apt-get clean     && rm -rf /var/lib/apt/lists/*
#14 CACHED

#15 [base 16/23] ADD ./config-files/hdfs-site.xml /opt/hadoop-2.6.4/etc/hadoop/
#15 CACHED

#16 [base 20/23] RUN tar xvfz spark-1.5.1-bin-hadoop2.6.tgz
#16 CACHED

#17 [base 12/23] RUN wget --no-check-certificate https://archive.apache.org/dist/hadoop/core/hadoop-2.6.4/hadoop-2.6.4.tar.gz
#17 CACHED

#18 [base 15/23] ADD ./config-files/core-site.xml /opt/hadoop-2.6.4/etc/hadoop/
#18 CACHED

#19 [base 10/23] WORKDIR /root/
#19 CACHED

#20 [base 11/23] WORKDIR /opt
#20 CACHED

#21 [base 18/23] ADD ./config-files/mapred-site.xml /opt/hadoop-2.6.4/etc/hadoop/
#21 CACHED

#22 [base 13/23] RUN tar xvfz hadoop-2.6.4.tar.gz
#22 CACHED

#23 [base 17/23] ADD ./config-files/yarn-site.xml /opt/hadoop-2.6.4/etc/hadoop/
#23 CACHED

#24 [base  5/23] RUN java -version
#24 CACHED

#25 [base  9/23] RUN chmod 600 /root/.ssh/id_ed25519
#25 CACHED

#26 [base 14/23] ADD ./config-files/hadoop-env.sh /opt/hadoop-2.6.4/etc/hadoop/
#26 CACHED

#27 [base 23/23] RUN mkdir -p /var/run/sshd
#27 CACHED

#28 [base] exporting to image
#28 exporting layers done
#28 writing image sha256:b3b16924a3e5b1fc23ba3e6ccb03e3a379b1125bf38ea2efc73af3d2028b14ae done
#28 naming to docker.io/library/sohad_base done
#28 DONE 0.0s
#0 building with "default" instance using docker driver

#1 [worker internal] load build definition from Dockerfile
#1 transferring dockerfile: 93B done
#1 DONE 0.0s

#2 [worker internal] load metadata for docker.io/library/sohad_base:latest
#2 DONE 0.0s

#3 [worker internal] load .dockerignore
#3 transferring context: 2B done
#3 DONE 0.0s

#4 [worker 1/1] FROM docker.io/library/sohad_base:latest
#4 CACHED

#5 [worker] exporting to image
#5 exporting layers done
#5 writing image sha256:dd01c1903103b72750dea9a00524f329cd2fdc3741b56d122980105b43a17752 done
#5 naming to docker.io/library/sohad_worker done
#5 DONE 0.0s
